\documentclass[10pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{verbatim}
\usepackage[demo]{graphicx}
\usepackage{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\R}{\ensuremath{\mathcal{R}}}
\newcommand{\mP}{\ensuremath{\mathcal{P}}}
\newcommand{\mM}{\ensuremath{\mathcal{M}}}
\newcommand{\ip}[2]{\ensuremath{\langle #1, #2 \rangle}}
\newcommand{\grad}{\nabla}
\newcommand{\one}{\ensuremath{\mathbf{1}}}
\newcommand{\co}{\mbox{co}}

\newcommand{\calL}{{\mathcal{L}}}
\newcommand{\rank}{{\calL(A)}}
\newcommand{\calO}{{\mathcal{O}}}
\newcommand{\calP}{{\mathcal{P}}}

\newcommand{\uni}{{\rank^n}}
\newcommand{\sca}{{\scr^{\alpha}}}
\newcommand{\sort}{\text{SORT}}
\newcommand{\phia}{\phi^{\alpha}}
\newcommand{\mmphia}{\mm^{\phia}}

\newcommand{\muhat}{\hat{\mu}}
\newcommand{\that}{\hat{\theta}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\eps}{\epsilon}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Euclidean Voting}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}

Let $A$ denote the set of $m$ alternatives, and $\rank$ and $\calO$ denote the space of rankings of alternatives and outcomes, respectively. Thus, $|\rank| = m!$, and let $k = |\calO|$. A profile $\pi \in \rank^n$ is a collection of votes (rankings). For any profile $\pi$, let $n(\pi,\sigma)$ denote the number of times $\sigma$ appears in $\pi$. Let $\sigma_1,\ldots,\sigma_{m!}$ denote a fixed reference order of the rankings in $\rank$. 

For any two profiles $\pi_1$ and $\pi_2$, let $\pi_1+\pi_2$ be the union profile such that $n(\pi_1+\pi_2,\sigma) = n(\pi_1,\sigma)+n(\pi_2,\sigma)$ for every $\sigma \in \rank$. Similarly, for any profile $\pi$, let $c \pi$ be the profile such that $n(c \pi,\sigma) = c \cdot n(\pi,\sigma)$ for every $\sigma \in \rank$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Voting Rule]
A voting rule (more technically, a social welfare function - SWF) $r : \rank^n \rightarrow \calP(\rank)\setminus\{\emptyset\}$ is a function that maps every profile of votes to a set of tied rankings. 
\end{definition}

Note that a voting rule can never output an empty set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Anonymity]
A voting rule $r$ is called \emph{anonymous} if it only depends on the number of times each ranking appears in the profile: for every profiles $\pi_1$ and $\pi_2$ such that $n(\pi_1,\sigma) = n(\pi_2,\sigma)$ for every $\sigma \in \rank$, $r(\pi_1) = r(\pi_2)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Rank-Distinguishability]
A voting rule $r$ is called \emph{rank-distinguishing} if it can distinguish between any two rankings: for every two rankings $\sigma, \sigma' \in \rank$ ($\sigma \neq \sigma'$), there exists a profile $\pi$ such that exactly one of $\sigma$ or $\sigma'$ is in $r(\pi)$. 
\end{definition}

In this paper, we only consider rank-distinguishing voting rules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Unanimity]
A voting rule $r$ is said to satisfy \emph{unanimity} if on every profile that consists of copies of a single ranking, it uniquely outputs that ranking: for every profile $\pi$ such that $n(\pi,\sigma) > 0$ for some $\sigma \in \rank$ and $n(\pi,\sigma') = 0$ for all $\sigma' \neq \sigma$, $r(\pi) = \{\sigma\}$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Neutrality]
Given any profile $\pi = (\sigma_1,\ldots,\sigma_n)$, let $\tau \pi = (\tau \sigma_1,\ldots,\tau \sigma_n)$ be the profile where each vote is permuted according to $\tau$. Similarly, given any set of rankings $S$, let $\tau S = \{\tau \sigma | \sigma \in S\}$. A voting rule $r$ is called \emph{neutral} if for every profile $\pi$ and permutation $\tau$, we have $r(\tau \pi) = \tau r(\pi)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Consistency] %and Weak Consistency]
A social welfare function $r$ is called \emph{consistent} (for rankings) if for every profiles $\pi_1$ and $\pi_2$ such that $r(\pi_1) \cap r(\pi_2) \neq \emptyset$, $r(\pi_1+\pi_2) = r(\pi_1) \cap r(\pi_2)$. %A social welfare function $r$ is called weakly consistent (for rankings) if for every profiles $\pi_1$ and $\pi_2$ such that $r(\pi_1) = r(\pi_2)$, we have $r(\pi_1+\pi_2) = r(\pi_1) = r(\pi_2)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Connectedness]
A voting rule $r$ is called \emph{connected} if for any two profiles $\pi_1$ and $\pi_2$ with $r(\pi_1) \cap r(\pi_2) \neq \emptyset$, there exist non-negative integers $c$ and $d$ such that $r(\pi_1) \cap r(c \pi_1 + d \pi_2) \neq \emptyset$ and $r(\pi_1) \neq r(c \pi_1 + d \pi_2)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Continuity]
Two profiles $\pi_1$ and $\pi_2$ satisfy $\pi_1 \approx \pi_2$ if they differ by one vote: for some $\sigma$ and $\sigma'$, $n(\pi_1,\sigma) = n(\pi_2,\sigma)-1$, $n(\pi_1,\sigma') = n(\pi_2,\sigma')+1$, and for every $\sigma \in \rank\setminus\{\sigma,\sigma'\}$, $n(\pi_1,\sigma) = n(\pi_2,\sigma)$. A voting rule $r$ is called \emph{continuous} if for every profile $\pi$ and ranking $\sigma$, $\sigma \notin r(\pi)$ implies that there exists integer $k$ such that for every profile $\pi' \approx k \pi$, $\sigma \notin r(\pi')$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background on Mean Proximity Rules and Generalized Scoring Rules}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Mean Proximity Rules (Zwicker~\cite{Zwicker08a})]
A voting rule is called a mean proximity rule if there exists an input embedding $\phi : \rank \rightarrow \mathbb{R}^k$ and an output embedding $\psi: \calO \rightarrow \mathbb{R}^k$ such that for any profile $\pi$ with $n$ votes, $r(\pi) = \argmin_{o \in \calO} \|\psi(o) - mean(\pi) \|$, where $mean(\pi) = \sum_{\sigma \in \rank} (n(\pi,\sigma)/n) \cdot \phi(\sigma)$ is the mean of the input embeddings of the votes in $\pi$ (along with multiplicity). 
\end{definition}

Note that a mean proximity rule is rank-distinguishing if and only if no embedding of the rule maps any two rankings to the same point in the Euclidean space. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Generalized Scoring Rules (Zwicker~\cite{Zwicker08a})]
A voting rule is called a generalized scoring rule if there exists a scoring function $s : \rank \times \calO \rightarrow \mathbb{R}$ such that for any profile $\pi$, $r(\pi) = \argmax_{o \in \calO} \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot s(\sigma,o)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For these two classes of voting rules, we have the following elegant equivalence theorem by Zwicker~\cite{Zwicker08a}. We reconstruct the proof since it is the foundation of several of our results.

\begin{proposition}[Corollary~4.2.3, Zwicker~\cite{Zwicker08a}]
Let $A = \{u_1,\ldots,u_l\}$ be a set of $l$ vectors in $\mathbb{R}^k$. Let $B = \{v_1,\ldots,v_p\}$ be a set of $p$ vectors in $\mathbb{R}^k$. Then, the discrete mean in $A$ of vectors in $B$ is the vector in $A$ that is closest to the Euclidean mean of vectors in $B$. That is,
$$
\argmin_{u_i \in A} \sum_{v_j \in B} \|u_i - v_j\|^2 = \argmin_{u_i \in A} \|u_i - (1/p) \cdot \sum_{v_j \in B} v_j\|.
$$
\label{prop:discrete-mean}
\end{proposition}

\begin{proposition}[Theorem 4.2.1, Zwicker~\cite{Zwicker08a}]
A voting rule is a mean proximity rule if and only if it is a generalized scoring rule.
\label{prop:equiv}
\end{proposition}
\begin{proof}[Proof (reconstructed)]
Take any mean proximity rule $r$. Let $\phi$ and $\psi$ be any input and output embeddings that generate $r$. Now, for any profile $\pi$,
\begin{align}
r(\pi)  &= \argmin_{o \in \calO} \|\psi(o)-mean(\pi)\| \nonumber\\
&= \argmin_{o \in \calO} \|\psi(o)-mean(\pi)\|^2 \nonumber\\
&= \argmin_{o \in \calO} \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot \|\psi(o)-\phi(\sigma)\|^2 \label{eqn:discrete-mean}\\
&= \argmax_{o \in \calO} \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot \left( -\|\psi(o)-\phi(\sigma)\|^2 \right), \label{eqn:equiv-main}
\end{align}
where Equation~\eqref{eqn:discrete-mean} follows by the well-known fact that the discrete mean, the member of a finite set that minimizes the sum of squares of Euclidean distances from given points, is the point in the finite set that is closest to the Euclidean mean (coordinate wise average) of the given points.  This is given as Corollary 4.2.3 in~\cite{Zwicker08a}. Now, we can see that $r$ is a generalized scoring rule by taking the score function $s(\sigma,o) = -\|\psi(o)-\phi(\sigma)\|^2$. 

For the other direction, take any generalized scoring rule $r$ and let $s$ be any score function that generates $r$. Then, let $\phi(\sigma) = (s(\sigma,o_1),\ldots,s(\sigma,o_k))$ where $\{o_1,\ldots,o_k\}$ is some fixed enumeration of $\calO$. Further, let $\psi(o_i) = e_i \in \mathbb{R}^k$ where the $i^{th}$ coordinate is $1$ and the rest are $0$. Then, for any profile $\pi$, we have
\begin{align*}
o_i \in r(\pi) &\Leftrightarrow \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot s(\sigma,o_i) \ge \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot s(\sigma,o), \forall o \in \calO\\
&\Leftrightarrow \langle mean(\pi), e_i \rangle \ge \langle mean(\pi), e_j \rangle, \forall 1 \le j \le k \\
&\Leftrightarrow \|mean(\pi) - e_i\| \ge \|mean(\pi) - e_j\|, \forall 1 \le j \le k\\
&\Leftrightarrow \|mean(\pi) - \psi(o_i)\|^2 \ge \|mean(\pi) - \psi(o_j)\|^2, \forall 1 \le j \le k,\\
&\Leftrightarrow \|mean(\pi) - \psi(o_i)\| \ge \|mean(\pi) - \psi(o_j)\|, \forall 1 \le j \le k,
\end{align*}
where the third transition follows since $\|e_j\| = 1$ for all $j$ (and thus, $\|mean(\pi) - \psi(o_j)\|^2 - \langle mean(\pi), e_j \rangle$ is constant for all $j$). Thus, $r$ is a mean proximity rule.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since mean proximity rules are equivalent to generalized scoring rules, here onwards we will use the term mean proximity rules out of the two. Further, Zwicker~\cite{Zwicker08b} shows that the set of voting rules that are \emph{consistent} and \emph{connected} is identical to the set of mean neat voting rules, which is a generalization of mean proximity rules. As a simple corollary, we have: 

\begin{proposition}
Any mean proximity rule is consistent and connected.
\end{proposition}

This implies that any voting rule that is not consistent (in the SWF sense) is not a mean proximity rule. In fact, we have the following.

\begin{lemma}[Proposition 1,2,5 and Theorem 3 of Conitzer et. al.~\cite{CRX09}]
All positional scoring rules and the Kemeny rule are mean proximity rules. However, Bucklin's rule, Copeland's rule, the maximin rule, the ranked pairs method, and STV are not mean proximity rules since they do not satisfy consistency (under any tie-breaking scheme). 
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mean Proximity SWFs and Symmetric Mean Proximity Rules}

In this paper, we are interested in social welfare functions (SWFs) that return a ranking, so $\calO = \rank$. From here onwards, unless mentioned otherwise, any voting rule we mention will be a social welfare function. In this case, the scoring function $s : \rank \times \rank \rightarrow \mathbb{R}$ describes the \emph{similarity} between two rankings. This special case was also defined and studied by Conitzer et. al.~\cite{CRX09} under the name \emph{simple ranking scoring functions} (SRSFs). 

\begin{definition}[Score Matrix]
For any SWF $r$ that is a generalized scoring rule (and hence SRSF), let $s$ be any score function that generates $r$. Let $\sigma_1,\ldots,\sigma_{m!}$ be any fixed enumeration of $\rank$. Then, the \emph{score matrix} of $r$ corresponding to the score function $s$, denoted $S$, is given by $S_{ij} = s(\sigma_i,\sigma_j)$, for $1 \le i \le m!$, $1 \le j \le m!$. 
\end{definition}

Now, given any profile $\pi$ of $n$ votes, we create the vector $y_{\pi} = [n(\pi,\sigma_1)/n,\ldots,n(\pi,\sigma_{m!})/n]^T$. It is easy to verify that the output of the rule would be $\sigma_k$ such that $k = \argmax_i (S\cdot y_{\pi})_i$. Using Proposition~\ref{prop:equiv}, we will now denote a mean proximity SWF by three equivalent representations: an embedding $\phi$, a scoring function $s$, and a score matrix $S$. Note that none of the three representations is unique. We will further say that the rule has embedding $\phi$, score function $s$, or score matrix $S$ to denote that it is one possible representation of the rule.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Symmetric Mean Proximity Rules}

While mean proximity rules capture several well-known SWFs, they capture many particularly bad voting rules as well. For example, we can imagine a mean proximity rule that has score function $s$ satisfying $s(\sigma,\sigma') > s(\sigma,\sigma)$ for some $\sigma,\sigma' \in \rank$. In this case, the rule would not output $\sigma$ even on the profile where all votes are $\sigma$, thus violating unanimity. To solve this problem, we propose a simple fix. We take the output embedding to be identical to the input embedding, i.e., $\psi = \phi$. Since the outcome space for SWFs is identical to the input space, this is a natural restriction. We call such rules symmetric mean proximity rules.

\begin{definition}[Symmetric Mean Proximity Rules]
A voting rule is called symmetric mean proximity rule if there exists a mean proximity representation of the rule where the input and the output embeddings are identical, i.e., $\psi = \phi$. 
\end{definition} 

It is easy to verify that this solves the problem of unanimity violation. It is immediate that for any profile $\pi$ consisting of copies of a single ranking $\sigma$, the ranking $\sigma$ is definitely in the output. Rank-distinguishability implies that no other ranking is in the output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
While there exist mean proximity rules violating unanimity, all symmetric mean proximity rules satisfy unanimity.
\end{lemma}

Moreover, symmetric mean proximity rules are not too restrictive, as they still capture the well-known SWFs captured by mean proximity rules.

\begin{lemma}
All positional scoring rules and the Kemeny rule are symmetric mean proximity rules.
\end{lemma}

To see this, observe that the constructions used in~\cite{Zwicker08a} to show that positional scoring rules and the Kemeny rule are mean proximity rules actually use identical input and output embeddings. Recall that mean proximity rules are identical to generalized scoring rules (Proposition~\ref{prop:equiv}). It is obvious to ask: \emph{What subclass of generalized scoring rules corresponds to symmetric mean proximity rules?} The answer turns out to be simple, yet interesting. Before that, we need the following definition.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Euclidean Distance Matrix]
An $n \times n$ matrix $A = (a_{ij})$ is called a \emph{Euclidean distance matrix} if there exist vectors $v_1,\ldots,v_n \in \mathbb{R}^k$ such that $a_{ij} = \|v_i-v_j\|^2$ for all $i,j$. 
\end{definition}

\begin{theorem}
A voting rule is a symmetric mean proximity rule if and only if it is a generalized scoring rule that has a score matrix whose negation is a Euclidean distance matrix. 
\label{thm:symm}
\end{theorem}
\begin{proof}
The ``if'' direction is simple. Given any generalized scoring rule $r$ and its score matrix $S$ such that $-S$ is a Euclidean distance matrix, we can find vectors $v_1,\ldots,v_{m!} \in \mathbb{R}^k$ such that $S_{ij} = -\|v_i-v_j\|^2$. Take $\phi(\sigma_i) = v_i$ for all $i$. Now, by Equation~\eqref{eqn:equiv-main} in the proof of Proposition~\ref{prop:equiv}, we have that 
$$
\argmax_{\sigma \in \rank} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot s(\sigma,\sigma') = \argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\|,
$$
that is, the rule is a symmetric mean proximity rule. 

For the ``only if'' direction, observe that given any symmetric mean proximity rule, the score matrix constructed in Equation~\eqref{eqn:equiv-main} in the proof of Proposition~\ref{prop:equiv} is indeed negation of a Euclidean distance matrix.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We showed that the class of symmetric mean proximity rules is slightly better than the class of all mean proximity rules. First, taking identical input and output embeddings is inherently more natural. Second, all symmetric mean proximity rules achieve unanimity, which is not the case for all mean proximity rules. Third, the restriction of a mean proximity rule being symmetric is not very restrictive, since the well-known mean proximity rules are symmetric anyway. In the next section, we analyze symmetric mean proximity rules that satisfy another very desired property -- neutrality. While neutrality is not very restrictive either (all voting rules of interest are neutral), we show that it adds significant structure to symmetric mean proximity rules. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neutrality in Symmetric Mean Proximity Rules and Linear Embeddings}

Conitzer et. al.~\cite{CRX09} showed that neutrality of a generalized scoring rule (i.e., a mean proximity rule) is equivalent to neutrality of its scoring function.
\begin{proposition}[Lemma 2, Conitzer et. al.~\cite{CRX09}]
A mean proximity rule is neutral if and only if all its scoring functions are neutral. A scoring function $s$ is neutral if it satisfies $s(\tau \sigma,\tau \sigma') = s(\sigma,\sigma')$ for all $\tau, \sigma, \sigma' \in \rank$.
\label{prop:gsr-neutral}
\end{proposition}

We are interested in symmetric mean proximity rules that satisfy neutrality. While Proposition~\ref{prop:gsr-neutral} exactly identifies the effect of neutrality on scoring functions, its effect on the embeddings representing the rule is not clear. Next, we investigate the structure imposed on these embeddings, ultimately leading to a full characterization.

\begin{lemma}
Let $r$ be any neutral voting rule. Let $\pi_{symm}$ be the profile containing each ranking exactly once, i.e., $n(\pi_{symm},\sigma) = 1$ for all $\sigma \in \rank$. Then, $r(\pi_{symm}) = \rank$. 
\label{lem:average-profile}
\end{lemma}
\begin{proof}
Let $r(\pi_{symm}) = T \subseteq \rank$. Suppose $T \neq \rank$, so there exists a $\sigma' \notin T$. Further, by definition of a voting rule, $T \neq \emptyset$. Thus, there exists a $\sigma \in T$. Now, take $\tau = \sigma' \sigma^{-1}$, where $\sigma^{-1}$ is the inverse of $\sigma$ in the symmetric group $S_m$. It is easy to see that $\tau \pi_{symm} = \pi_{symm}$. Hence, $r(\tau \pi_{symm}) = r(\pi_{symm}) = T$. However, $\sigma' = \sigma' \sigma^{-1} \sigma = \tau \sigma \in \tau r(\pi_{symm})$. Thus, $\tau r(\pi_{symm}) \neq r(\tau \pi_{symm})$. This implies that $r$ violates neutrality, a contradiction. 
\end{proof}

Take any neutral rule $r$ and let $\phi$ be its embedding. Inspired from Lemma~\ref{lem:average-profile}, define $\phi_{avg} = (1/{m!}) \cdot \sum_{\sigma \in \rank} \phi(\sigma)$ to be the average of all embeddings, i.e., $mean(\pi_{symm})$. Then, Lemma~\ref{lem:average-profile} indicates that $\|\phi(\sigma)-\phi_{avg}\|$ must be independent of $\sigma$ (since all rankings need to be tied). Further, note that translating and scaling all embeddings in the Euclidean space does not change the voting rule. Thus, we can take the new embedding $\phi'(\sigma) = (\phi(\sigma)-\phi_{avg})/\|\phi(\sigma)-\phi_{avg}\|$ that would also represent the same rule (note that crucially, all embeddings are being scaled by the same constant). Also, $\phi'$ satisfies that all embeddings have unit length and their average is the origin. Thus, we have the following.

\begin{lemma}
For any neutral symmetric mean proximity rule $r$, there exists an embedding $\phi$ such that $\|\phi(\sigma)\| = 1$ for every $\sigma \in \rank$, and $\phi_{avg} = (1/{m!}) \cdot \sum_{\sigma \in \rank} \phi(\sigma) = 0$.
\label{lem:normalized-embedding}
\end{lemma}

We call such an embedding a normalized embedding of the rule. Using this, a result for neutral symmetric mean proximity rules that further adds to Theorem~\ref{thm:symm}. 
\begin{definition}[Gramian Matrix]
An $n \times n$ matrix $A = (a_{ij})$ is called Gramian if there exist vectors $v_1,\ldots,v_n \in \mathbb{R}^k$ such that $a_{ij} = \langle v_i,v_j \rangle$ for all $i,j$. 
\end{definition}

%Note that if matrix $B$ consists of $v_1,\ldots,v_n$ as its rows, then $A = B B^T$. It is well-known that $rank(A) = rank(B)$ in this case. 
Further, Gramian matrices satisfy the following elegant characterization.
\begin{proposition}
A matrix is Gramian if and only if it is positive semidefinite.
\label{prop:gramian}
\end{proposition}

\begin{theorem}
A voting rule is a neutral symmetric mean proximity rule if and only if it is a neutral generalized scoring rule for which there exists a positive semidefinite score matrix. 
\label{thm:psd}
\end{theorem}
\begin{proof}
Let $r$ be any neutral symmetric mean proximity rule. Then, by Lemma~\ref{lem:normalized-embedding}, it has a normalized embedding $\phi$. For any profile $\pi$ with $n$ votes,
\begin{align}
r(\pi) &= \argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\| \nonumber\\
&= \argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\|^2 \nonumber\\
&= \argmin_{\sigma \in \rank} \|\phi(\sigma)\|^2 + \|mean(\pi)\|^2 - 2 \cdot \langle \phi(\sigma),mean(\pi) \rangle  \nonumber\\
&= \argmin_{\sigma \in \rank} 1+\|mean(\pi)\|^2 - 2 \cdot \langle \phi(\sigma), mean(\pi) \rangle \nonumber\\
&= \argmax_{\sigma \in \rank} \langle \phi(\sigma), (1/n) \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot \phi(\sigma') \rangle \nonumber\\
&= \argmax_{\sigma \in \rank} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot \langle \phi(\sigma),\phi(\sigma') \rangle. \label{eqn:equiv-dist-inner-product}
\end{align}
From the above formulation, it is evident that the scoring function $s(\sigma,\sigma') = \langle \phi(\sigma),\phi(\sigma') \rangle$ represents rule $r$. This corresponds to a Gramian score matrix, which is therefore positive semidefinite (Proposition~\ref{prop:gramian}).

On the contrary, take any neutral generalized scoring rule $r$ with a positive semidefinite (and hence Gramian) score matrix $S$. Thus, there exist vectors $v_1,\ldots,v_{m!} \in \mathbb{R}^k$ such that $S_{ij} = \langle v_i,v_j \rangle$. Take $\phi(\sigma_i) = v_i$ for all $i$. Using Equation~\eqref{eqn:equiv-dist-inner-product}, it is clear that $r$ is also the symmetric mean proximity rule with embedding $\phi$. Further, $r$ is neutral by assumption. 
\end{proof}

We can easily check that by taking a normalized embedding, we can achieve a score matrix with further restrictions.
\begin{lemma}
Any neutral symmetric mean proximity rule $r$ has a score matrix $S$ that satisfies the following.
\begin{enumerate}
\item $S$ is positive semidefinite.
\item $S_{ii} = S_{jj}$ for all $i,j$, i.e., all diagonal entries are equal.
\item $S_{ii} > S_{ij}$ for all $i,j$, i.e., the largest element in any row is its diagonal element.
\item $\sum_j S_{ij}$ is independent of $i$, i.e., the sum of elements in each row is the same.
\end{enumerate}
\end{lemma}
Note that the third condition follows from unanimity of symmetric mean proximity rules, and the last condition follows from Lemma~\ref{lem:average-profile}. In the next section, we introduce a general class of embeddings that all achieve neutrality, and use the structure established here to show that they in fact generate all neutral symmetric mean proximity rules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%       POLISHED TILL HERE        %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Embeddings and Neutrality}

\begin{definition}[Linear Embeddings]
$\phi(\tau \sigma) = R_{\tau} \cdot \phi(\sigma)$. 
\end{definition}

Motivation: While mean proximity rules / generalized scoring rules capture many nice voting rules, they capture bad ones too. In particular, if we have $s(\sigma,\sigma') > s(\sigma,\sigma)$ for any $\sigma$ and $\sigma'$, then the rule will not return $\sigma$ on the profile where all votes are $\sigma$, thus violating strong unanimity. While linear embeddings solve this problem and achieve many other desirable properties, they are quite unrestrictive - they still capture all positional scoring rules and the Kemeny rule.

\begin{theorem}
Any mean proximity rule that has a representation using a linear embedding satisfies strong unanimity and neutrality. 
\end{theorem}

In addition, we can now define a distance function $d(\sigma,\sigma') = \|\phi(\sigma)-\phi(\sigma')\|$. It is easy to show that this, in addition to being a distance metric, is also left-invariant. Additionally, if we take the linear embedding that generates the Kemeny rule, then the corresponding distance function becomes the squaure root of the KT distance. This demonstrates that the KT distance is actually the square of a more natural Euclidean distance metric. In this sense, the Kemeny rule is a mean rule, rather than a median rule, and so are all positional scoring rules. 

\begin{theorem}
A symmetric mean proximity rule is neutral if and only if there exists a representation $\phi$ of the rule that is linear. 
\label{thm:neutrality-linear-embedding}
\end{theorem}

\section{Euclidean Embeddings and Dimensions}

In this section, we analyze the following general question. \emph{Given a symmetric mean proximity rule $r$, what is the minimum dimension of any embedding that represents $r$?}. 

\begin{conjecture}
Let $S$ and $S'$ be two score matrices with rows $\{r_i\}$ and $\{r'_i\}$. Then, $S$ and $S'$ correspond to the same generalized scoring rule if and only if there exist $a \in \mathbb{R}$ and $b \in \mathbb{R}^{m!}$ such that for every $i$, $r'_i = a \cdot r_i + b$. 
\end{conjecture}

\begin{theorem}
The minimum dimension required for any positional scoring rule is $m-1$. 
\end{theorem}

\subsection{Borda Rule}
In this section, we analyze the $m-1$ dimension embeddings of the Borda rule. 

{\bf $\mathbf{3}$ alternatives:} Zwicker~\cite{Zwicker08a} informally demonstrated that for $3$ alternatives, any embedding that represents the Borda rule embeds the $6$ possible rankings at the corners of a regular hexagon as shown in Figure~\ref{fig:borda-3alt}. In fact, it is exactly the \emph{permutahedron} of the symmetric group $S_3$ where each ranking is connected to two rankings that are obtained by two possible adjacent swaps. Thus, neighbouring rankings have \emph{Kendall Tau} distance $1$ from each other. Note that there are hexagons that connect rankings at KT distance greater than $1$, but those do not correspond to the Borda rule.

\begin{figure}
\centering
\begin{subfigure}[$3$ alternatives]
  %\includegraphics[width=.4\linewidth]{Borda3}
  {\rule{3cm}{3cm}}
  \label{fig:borda-3alt}
\end{subfigure}%
\begin{subfigure}[$4$ alternatives]
  %\includegraphics[width=.4\linewidth]{Borda4}
  {\rule{3cm}{3cm}}
  \label{fig:borda-4alt}
\end{subfigure}%
\caption{Embeddings for the Borda Rule}
\label{fig:borda}
\end{figure}

{\bf $\mathbf{4}$ alternatives:} Interestingly, the observation that the Borda rule embeds rankings to the vertices of the regular polytope of the permutahedron carries over to the case of $4$ alternatives. Thus, each ranking is connected to $3$ rankings that are obtained by performing one of the three possible adjacent swaps. The polytope of the permutahedron of $S_4$, shown in Figure~\ref{fig:borda-4alt}, consists of $8$ hexagons and $6$ squares. Each hexagon contains the $6$ rankings that are obtained by either keeping the first alternative constant or the last alternative constant. Note that we can forget the alternative that is constant, and the hexagon exactly matches the embeddings of the rankings over the remaining $3$ alternatives. The $6$ squares each contain $4$ rankings that are obtained by swapping the first two or the last two alternatives. Thus, each square has the following form: 
$$
a \succ b \succ c \succ d \;\longrightarrow\; a \succ b \succ d \succ c \;\longrightarrow\; b \succ a \succ d \succ c \;\longrightarrow\; b \succ a \succ c \succ d.
$$
{\bf Replace this by a figure of a square, similarly draw a hexagon}

For better understanding of permutahedrons, refer to Crisman~\cite{Crisman}. 


\section{Connections to other approaches}
\subsection{Axiomatic}
Consistency, continuity, anonymity, neutrality already described above. 

{\bf Question:} When is it PM-c (related to the Euclidean distance being MC?), monotonic, majority for rankings?

\subsection{DR}
Consensus = Strong unanimity. Distance = Square of Euclidean distance. Votewise DR rules. 

Square is not always a distance metric $\rightarrow$ Bad. More meaningfully, define votewise DR rules by sum of squares of distances rather than sum of distances. . 

\subsection{MLE}
Take $\Pr[\sigma | \sigma^*] \propto e^{\|\phi(\sigma)-\phi(\sigma^*)\|^2}$. Not always Mallows since square is not always a metric. But actually better $\rightarrow$ Gaussian. 

Neutrality $\Rightarrow$ Normalization independent of the true ranking $\Rightarrow$ Linear mean proximity rule is the MLE for this model. 

{\bf Question:} Efficient sampling?

{\bf Question:} Anything interesting for the Gaussian distributions that connect to PSR?

\section{Research Questions}
\begin{enumerate}
\item What shape, and what corresponding voting rule do we get if we replace the KT distance by some other distance in the permutahedron?
\item Proving a lower bound on the dimensions for the Kemeny rule
\item Conjecture by Conitzer et. al.~\cite{CRX09}: Consistent $+$ continuous $+$ neutral $\Leftrightarrow$ Neutral SRSF
\item What are the equivalence classes of $\phi$ that lead to the same voting rule?
\item What about notions of consensus in Euclidean spaces other than mean $\rightarrow$ e.g., minimize maximum distance (everyone ``lets go'' equally)?
\end{enumerate}


\begin{comment}
Mean proximity rule / generalized scoring rule / SRSF - Neutral $\Rightarrow$ SRSF iff MLE
{\bf Question:} (Linear) Mean Proximity Rules - Captures all ``pairwise comparison scoring rules''?
\end{comment}

\bibliographystyle{plain}
\bibliography{abbshort,ultimate}
\end{document}