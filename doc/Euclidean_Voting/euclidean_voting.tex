\documentclass[10pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{environ}
\usepackage{color}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{verbatim}
\usepackage[demo]{graphicx}
\usepackage{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\R}{\ensuremath{\mathcal{R}}}
\newcommand{\mP}{\ensuremath{\mathcal{P}}}
\newcommand{\mM}{\ensuremath{\mathcal{M}}}
\newcommand{\ip}[2]{\ensuremath{\langle #1, #2 \rangle}}
\newcommand{\grad}{\nabla}
\newcommand{\one}{\ensuremath{\mathbf{1}}}
\newcommand{\co}{\mbox{co}}

\newcommand{\calL}{{\mathcal{L}}}
\newcommand{\rank}{{\calL(A)}}
\newcommand{\calO}{{\mathcal{O}}}
\newcommand{\calP}{{\mathcal{P}}}

\newcommand{\uni}{{\rank^n}}
\newcommand{\sca}{{\scr^{\alpha}}}
\newcommand{\sort}{\text{SORT}}
\newcommand{\phia}{\phi^{\alpha}}
\newcommand{\mmphia}{\mm^{\phia}}

\newcommand{\muhat}{\hat{\mu}}
\newcommand{\that}{\hat{\theta}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\eps}{\epsilon}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcount\Comments
\Comments=1
\newcommand{\kibitz}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\csl}[1]{\kibitz{blue} {[SL: #1]}}
\newcommand{\cns}[1]{\kibitz{red} {[NS: #1]}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Euclidean Voting}
\author{S\'{e}bastien Lahaie\\Microsoft Research New York, USA\\slahaie@microsoft.com \and Nisarg Shah\\Carnegie Mellon University, USA\\nkshah@cs.cmu.edu}
\date{}
\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}

Let $A$ denote the set of $m$ alternatives, and $\rank$ and $\calO$ denote the space of rankings of alternatives and outcomes, respectively. Thus, $|\rank| = m!$, and let $k = |\calO|$. A profile $\pi \in \rank^n$ is a collection of votes (rankings). For any profile $\pi$, let $n(\pi,\sigma)$ denote the number of times $\sigma$ appears in $\pi$. Let $\sigma_1,\ldots,\sigma_{m!}$ denote a fixed reference order of the rankings in $\rank$. 

For any two profiles $\pi_1$ and $\pi_2$, let $\pi_1+\pi_2$ be the union profile such that $n(\pi_1+\pi_2,\sigma) = n(\pi_1,\sigma)+n(\pi_2,\sigma)$ for every $\sigma \in \rank$. Similarly, for any profile $\pi$, let $c \pi$ be the profile such that $n(c \pi,\sigma) = c \cdot n(\pi,\sigma)$ for every $\sigma \in \rank$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Voting Rule]
A \emph{voting rule} (more technically, a social welfare function - SWF) $r : \rank^n \rightarrow \calP(\rank)\setminus\{\emptyset\}$ is a function that maps every profile of votes to a set of tied rankings. 
\end{definition}

Note that a voting rule can never output an empty set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Anonymity]
A voting rule $r$ is called \emph{anonymous} if it only depends on the number of times each ranking appears in the profile: for every profiles $\pi_1$ and $\pi_2$ such that $n(\pi_1,\sigma) = n(\pi_2,\sigma)$ for every $\sigma \in \rank$, $r(\pi_1) = r(\pi_2)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Rank-Distinguishability]
A voting rule $r$ is called \emph{rank-distinguishing} if it can distinguish between any two rankings: for every two rankings $\sigma, \sigma' \in \rank$ ($\sigma \neq \sigma'$), there exists a profile $\pi$ such that exactly one of $\sigma$ or $\sigma'$ is in $r(\pi)$. 
\end{definition}

In this paper, we only consider rank-distinguishing voting rules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Unanimity]
A voting rule $r$ is said to satisfy \emph{unanimity} if on every profile that consists of copies of a single ranking, it uniquely outputs that ranking: for every profile $\pi$ such that $n(\pi,\sigma) > 0$ for some $\sigma \in \rank$ and $n(\pi,\sigma') = 0$ for all $\sigma' \neq \sigma$, $r(\pi) = \{\sigma\}$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Neutrality]
Given any profile $\pi = (\sigma_1,\ldots,\sigma_n)$, let $\tau \pi = (\tau \sigma_1,\ldots,\tau \sigma_n)$ be the profile where each vote is permuted according to $\tau$. Similarly, given any set of rankings $S$, let $\tau S = \{\tau \sigma | \sigma \in S\}$. A voting rule $r$ is called \emph{neutral} if for every profile $\pi$ and permutation $\tau$, we have $r(\tau \pi) = \tau r(\pi)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Consistency] %and Weak Consistency]
A social welfare function $r$ is called \emph{consistent} (for rankings) if for every profiles $\pi_1$ and $\pi_2$ such that $r(\pi_1) \cap r(\pi_2) \neq \emptyset$, $r(\pi_1+\pi_2) = r(\pi_1) \cap r(\pi_2)$. %A social welfare function $r$ is called weakly consistent (for rankings) if for every profiles $\pi_1$ and $\pi_2$ such that $r(\pi_1) = r(\pi_2)$, we have $r(\pi_1+\pi_2) = r(\pi_1) = r(\pi_2)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Connectedness]
A voting rule $r$ is called \emph{connected} if for any two profiles $\pi_1$ and $\pi_2$ with $r(\pi_1) \cap r(\pi_2) \neq \emptyset$, there exist non-negative integers $c$ and $d$ such that $r(\pi_1) \cap r(c \pi_1 + d \pi_2) \neq \emptyset$ and $r(\pi_1) \neq r(c \pi_1 + d \pi_2)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cns{Check if this is equivalent/related to continuity defined by Conitzer et. al.~\cite{CRX09}. Then we can add that to Proposition~\ref{prop:properties}.}
\begin{definition}[Continuity]
Two profiles $\pi_1$ and $\pi_2$ satisfy $\pi_1 \approx \pi_2$ if they differ by one vote: for some $\sigma$ and $\sigma'$, $n(\pi_1,\sigma) = n(\pi_2,\sigma)-1$, $n(\pi_1,\sigma') = n(\pi_2,\sigma')+1$, and for every $\sigma \in \rank\setminus\{\sigma,\sigma'\}$, $n(\pi_1,\sigma) = n(\pi_2,\sigma)$. A voting rule $r$ is called \emph{continuous} if for every profile $\pi$ and ranking $\sigma$, $\sigma \notin r(\pi)$ implies that there exists integer $k$ such that for every profile $\pi' \approx k \pi$, $\sigma \notin r(\pi')$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background on Mean Proximity Rules and Generalized Scoring Rules}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Mean Proximity Rules (Zwicker~\cite{Zwicker08a})]
A voting rule is called a \emph{mean proximity rule} if there exists an input embedding $\phi : \rank \rightarrow \mathbb{R}^k$ and an output embedding $\psi: \calO \rightarrow \mathbb{R}^k$ such that for any profile $\pi$ with $n$ votes, $r(\pi) = \argmin_{o \in \calO} \|\psi(o) - mean(\pi) \|$, where $mean(\pi) = \sum_{\sigma \in \rank} (n(\pi,\sigma)/n) \cdot \phi(\sigma)$ is the mean of the input embeddings of the votes in $\pi$ (along with multiplicity). 
\end{definition}

Note that a mean proximity rule is rank-distinguishing if and only if no embedding of the rule maps any two rankings to the same point in the Euclidean space. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Generalized Scoring Rules (Zwicker~\cite{Zwicker08a})]
A voting rule is called a \emph{generalized scoring rule} if there exists a scoring function $s : \rank \times \calO \rightarrow \mathbb{R}$ such that for any profile $\pi$, $r(\pi) = \argmax_{o \in \calO} \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot s(\sigma,o)$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{proposition}[Corollary~4.2.3, Zwicker~\cite{Zwicker08a}]
%Let $A = \{u_1,\ldots,u_l\}$ be a set of $l$ vectors in $\mathbb{R}^k$. Let $B = \{v_1,\ldots,v_p\}$ be a set of $p$ vectors in $\mathbb{R}^k$. Then, the discrete mean in $A$ of vectors in $B$ is the vector in $A$ that is closest to the Euclidean mean of vectors in $B$. That is,
%$$
%\argmin_{u_i \in A} \sum_{v_j \in B} \|u_i - v_j\|^2 = \argmin_{u_i \in A} \|u_i - (1/p) \cdot \sum_{v_j \in B} v_j\|.
%$$
%\label{prop:discrete-mean}
%\end{proposition}

For these two classes of voting rules, we have an elegant equivalence theorem by Zwicker~\cite{Zwicker08a}. The theorem uses an important result shown below.

\begin{proposition}[Zwicker~\cite{Zwicker08a}]
For any embeddings $\phi,\psi : \rank \rightarrow \mathbb{R}^k$ and any profile $\pi$,
$$
\argmin_{o \in \calO} \|\psi(o)-mean(\pi)\| = \argmin_{o \in \calO} \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot \|\psi(o)-\phi(\sigma)\|^2.
$$ 
\label{prop:MPR-GSR-conversion}
\end{proposition}
\begin{proof}[Proof (reconstructed)]
\begin{align*}
r(\pi)  &= \argmin_{o \in \calO} \|\psi(o)-mean(\pi)\| \\
&= \argmin_{o \in \calO} \|\psi(o)-mean(\pi)\|^2 \\
&= \argmin_{o \in \calO} \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot \|\psi(o)-\phi(\sigma)\|^2 \\
&= \argmax_{o \in \calO} \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot \left( -\|\psi(o)-\phi(\sigma)\|^2 \right), 
\end{align*}
where the third transition follows by Corollary~4.2.3 of~\cite{Zwicker08a}.%Proposition~\ref{prop:discrete-mean}. 
\end{proof}

We now give the equivalence theorem, and reconstruct its proof since it serves as a foundation of several of our results.
\begin{proposition}[Theorem 4.2.1, Zwicker~\cite{Zwicker08a}]
A voting rule is a mean proximity rule if and only if it is a generalized scoring rule.
\label{prop:equiv}
\end{proposition}
\begin{proof}[Proof (reconstructed)]
Take any mean proximity rule $r$. Let $\phi$ and $\psi$ be any input and output embeddings that generate $r$. Using Proposition~\ref{prop:MPR-GSR-conversion}, we can see that $r$ is a generalized scoring rule with the score function $s(\sigma,o) = -\|\psi(o)-\phi(\sigma)\|^2$. 

For the other direction, take any generalized scoring rule $r$ and let $s$ be any score function that generates $r$. Then, let $\phi(\sigma) = (s(\sigma,o_1),\ldots,s(\sigma,o_k))$ where $\{o_1,\ldots,o_k\}$ is some fixed enumeration of $\calO$. Further, let $\psi(o_i) = e_i \in \mathbb{R}^k$ where the $i^{th}$ coordinate is $1$ and the rest are $0$. Then, for any profile $\pi$, we have
\begin{align*}
o_i \in r(\pi) &\Leftrightarrow \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot s(\sigma,o_i) \ge \sum_{\sigma \in \rank} n(\pi,\sigma) \cdot s(\sigma,o), \forall o \in \calO\\
&\Leftrightarrow \langle mean(\pi), e_i \rangle \ge \langle mean(\pi), e_j \rangle, \forall 1 \le j \le k \\
&\Leftrightarrow \|mean(\pi) - e_i\| \ge \|mean(\pi) - e_j\|, \forall 1 \le j \le k\\
&\Leftrightarrow \|mean(\pi) - \psi(o_i)\|^2 \ge \|mean(\pi) - \psi(o_j)\|^2, \forall 1 \le j \le k,\\
&\Leftrightarrow \|mean(\pi) - \psi(o_i)\| \ge \|mean(\pi) - \psi(o_j)\|, \forall 1 \le j \le k,
\end{align*}
where the third transition follows since $\|e_j\| = 1$ for all $j$ (and thus, $\|mean(\pi) - \psi(o_j)\|^2 - \langle mean(\pi), e_j \rangle$ is constant for all $j$). Thus, $r$ is a mean proximity rule.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since mean proximity rules are equivalent to generalized scoring rules, here onwards we will use the term mean proximity rules out of the two. Further, Zwicker~\cite{Zwicker08b} shows that the set of voting rules that are \emph{consistent} and \emph{connected} is identical to the set of mean neat voting rules, which is a generalization of mean proximity rules. As a simple corollary, we have: 

\cns{Check if continuity in~\cite{CRX09} is same as continuity in~\cite{Zwicker08a}. If so, take the definition from~\cite{Zwicker08a}, else from~\cite{CRX09}.}
\begin{proposition}[\cite{Zwicker08b}]
Any mean proximity rule is consistent, connected, continuous, and anonymous.
\label{prop:properties}
\end{proposition}

This implies that any voting rule that is not consistent (in the SWF sense) is not a mean proximity rule. In fact, we have the following.

\begin{lemma}[Proposition 1,2,5 and Theorem 3 of Conitzer et. al.~\cite{CRX09}]
All positional scoring rules and the Kemeny rule are mean proximity rules. However, Bucklin's rule, Copeland's rule, the maximin rule, the ranked pairs method, and STV are not mean proximity rules since they do not satisfy consistency (under any tie-breaking scheme). 
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Symmetric Mean Proximity Rules}

In this paper, we are interested in social welfare functions (SWFs) that return a ranking, so $\calO = \rank$. From here onwards, unless mentioned otherwise, any voting rule we mention will be a social welfare function. In this case, the scoring function $s : \rank \times \rank \rightarrow \mathbb{R}$ describes the \emph{similarity} between two rankings. This special case was also defined and studied by Conitzer et. al.~\cite{CRX09} under the name \emph{simple ranking scoring functions} (SRSFs). 

\begin{definition}[Score Matrix]
For any SWF $r$ that is a generalized scoring rule (hence, an SRSF), let $s$ be any score function that generates $r$. Let $\sigma_1,\ldots,\sigma_{m!}$ be any fixed enumeration of $\rank$. Then, the \emph{score matrix} of $r$ corresponding to the score function $s$, denoted $S$, is given by $S_{ij} = s(\sigma_i,\sigma_j)$, for $1 \le i \le m!$, $1 \le j \le m!$. 
\end{definition}

Given any profile $\pi$ of $n$ votes, create $y_{\pi} = [n(\pi,\sigma_1)/n,\ldots,n(\pi,\sigma_{m!})/n]^T$. It is easy to verify that the output of the rule would be $\sigma_k$ such that $k = \argmax_i (S\cdot y_{\pi})_i$. 

Using Proposition~\ref{prop:equiv}, we can denote a mean proximity SWF by three equivalent representations: i) an embedding $\phi$, ii) a scoring function $s$, and iii) a score matrix $S$. Note that none of the three representations is unique. %We will further say that the rule has embedding $\phi$, score function $s$, or score matrix $S$ to denote that it is one possible representation of the rule.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Symmetric Mean Proximity Rules}

While mean proximity rules capture several well-known SWFs, they capture bad voting rules as well. For example, we can imagine a mean proximity rule that has score function $s$ satisfying $s(\sigma,\sigma') > s(\sigma,\sigma)$ for some $\sigma,\sigma' \in \rank$. In this case, the rule would not output $\sigma$ even on the profile where all votes are $\sigma$, thus violating unanimity. To solve this problem, we propose a simple fix. We take the output embedding to be identical to the input embedding, i.e., $\psi = \phi$. Since the outcome space for SWFs is identical to the input space, this is a natural restriction. We call such rules symmetric mean proximity rules.

\begin{definition}[Symmetric Mean Proximity Rules]
A voting rule is called \emph{symmetric mean proximity rule} if there exists a mean proximity representation of the rule where the input and the output embeddings are identical, i.e., $\psi = \phi$. 
\end{definition} 

It is easy to verify that this solves the problem of unanimity violation. It is immediate that for any profile $\pi$ consisting of copies of a single ranking $\sigma$, the ranking $\sigma$ is definitely in the output. Rank-distinguishability implies that no two rankings are embedded to the same point, hence no other ranking is in the output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
While there exist mean proximity rules violating unanimity, all symmetric mean proximity rules satisfy unanimity.
\end{lemma}

Moreover, symmetric mean proximity rules are not too restrictive, as they still capture the well-known SWFs captured by mean proximity rules.

\begin{lemma}
All positional scoring rules and the Kemeny rule are symmetric mean proximity rules.
\label{lem:symmetric-captures}
\end{lemma}

To see this, observe that the constructions in~\cite{Zwicker08a} for positional scoring rules and the Kemeny rule actually use identical input and output embeddings. Recall that mean proximity rules are identical to generalized scoring rules (Proposition~\ref{prop:equiv}). It is obvious to ask: \emph{What subclass of generalized scoring rules corresponds to symmetric mean proximity rules?} The answer turns out to be simple, yet interesting. Before that, we need the following definition.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Euclidean Distance Matrix]
An $n \times n$ matrix $A = (a_{ij})$ is called a \emph{Euclidean distance matrix} if there exist vectors $v_1,\ldots,v_n \in \mathbb{R}^k$ such that $a_{ij} = \|v_i-v_j\|^2$ for all $i,j$. 
\end{definition}

\begin{theorem}
A voting rule is a symmetric mean proximity rule if and only if it is a generalized scoring rule that has a score matrix whose negation is a Euclidean distance matrix. 
\label{thm:symm}
\end{theorem}
\begin{proof}
The ``if'' direction is simple. Given any generalized scoring rule $r$ and its score matrix $S$ such that $-S$ is a Euclidean distance matrix, we can find vectors $v_1,\ldots,v_{m!} \in \mathbb{R}^k$ such that $S_{ij} = -\|v_i-v_j\|^2$. Take $\phi(\sigma_i) = v_i$ for all $i$. Now, by Proposition~\ref{prop:MPR-GSR-conversion}, 
$$
\argmax_{\sigma \in \rank} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot s(\sigma,\sigma') = \argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\|,
$$
that is, the rule is a symmetric mean proximity rule. 

For the ``only if'' direction, observe that given any symmetric mean proximity rule, the score matrix constructed in the proof of Proposition~\ref{prop:equiv} is indeed negation of a Euclidean distance matrix.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We showed that the class of symmetric mean proximity rules is slightly better than the class of all mean proximity rules. First, taking identical input and output embeddings is inherently natural. Second, all symmetric mean proximity rules achieve unanimity. Third, the restriction of a mean proximity rule being symmetric is mild; all well-known mean proximity rules are symmetric. In the next section, we analyze symmetric mean proximity rules that satisfy another highly desired property -- neutrality. While neutrality is also mild (all voting rules of interest are neutral), we show that it adds significant structure to symmetric mean proximity rules. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neutral Symmetric Mean Proximity Rules}

Conitzer et. al.~\cite{CRX09} showed that neutrality of a generalized scoring rule (i.e., a mean proximity rule) can be translated to neutrality of its scoring function.
\begin{proposition}[Lemma 2, Conitzer et. al.~\cite{CRX09}]
A mean proximity rule is neutral if and only if it has a neutral scoring function. A scoring function $s$ is neutral if it satisfies $s(\tau \sigma,\tau \sigma') = s(\sigma,\sigma')$ for all $\tau, \sigma, \sigma' \in \rank$.
\label{prop:gsr-neutral}
\end{proposition}

We are interested in symmetric mean proximity rules that satisfy neutrality. In Theorem~\ref{thm:symm}, we identified the subclass of generalized scoring rules that corresponds to symmetric mean proximity rules. An immediate question is: \emph{What subclass of generalized scoring rules corresponds to neutral symmetric mean proximity rules?.} To answer this, we first need the following result.

\begin{lemma}
Let $r$ be any neutral voting rule. Let $\pi_{symm}$ be the profile containing each ranking exactly once, i.e., $n(\pi_{symm},\sigma) = 1$ for all $\sigma \in \rank$. Then, $r(\pi_{symm}) = \rank$. 
\label{lem:average-profile}
\end{lemma}
\begin{proof}
Let $r(\pi_{symm}) = T \subseteq \rank$. Suppose $T \neq \rank$, so there exists a $\sigma' \notin T$. Further, by definition of a voting rule, $T \neq \emptyset$. Thus, there exists a $\sigma \in T$. Now, take $\tau = \sigma' \sigma^{-1}$, where $\sigma^{-1}$ is the inverse of $\sigma$ in the symmetric group $S_m$. It is easy to see that $\tau \pi_{symm} = \pi_{symm}$. Hence, $r(\tau \pi_{symm}) = r(\pi_{symm}) = T$. However, $\sigma' = \sigma' \sigma^{-1} \sigma = \tau \sigma \in \tau r(\pi_{symm})$. Thus, $\tau r(\pi_{symm}) \neq r(\tau \pi_{symm})$. This implies that $r$ violates neutrality, a contradiction. 
\end{proof}

Take any neutral symmetric mean proximity rule $r$, and let $\phi$ be its embedding. Inspired from Lemma~\ref{lem:average-profile}, define $\phi_{avg} = (1/{m!}) \cdot \sum_{\sigma \in \rank} \phi(\sigma) = mean(\pi_{symm})$ to be the average of all embeddings. Then, Lemma~\ref{lem:average-profile} indicates that $\|\phi(\sigma)-\phi_{avg}\|$ must be independent of $\sigma$ (since all rankings need to be tied). Further, note that translating and scaling all embeddings in the Euclidean space does not change a symmetric mean proximity rule. Thus, the embedding $\phi'(\sigma) = (\phi(\sigma)-\phi_{avg})/\|\phi(\sigma)-\phi_{avg}\|$ represents the same mean proximity rule.\footnote{Crucially, all embeddings are scaled by the same constant.} Also, under $\phi'$, all embeddings have unit length and their average is the origin. 

\begin{lemma}
For any neutral symmetric mean proximity rule $r$, there exists an embedding $\phi$ such that $\|\phi(\sigma)\| = 1$ for every $\sigma \in \rank$, and $\phi_{avg} = (1/{m!}) \cdot \sum_{\sigma \in \rank} \phi(\sigma) = 0$.
\label{lem:normalized-embedding}
\end{lemma}

We call such an embedding a \emph{normalized embedding}. We can now identify the subclass of generalized scoring rules that corresponds to neutral symmetric mean proximity rules. 
\begin{definition}[Gramian Matrix]
An $n \times n$ matrix $A = (a_{ij})$ is called a \emph{Gramian matrix} if there exist vectors $v_1,\ldots,v_n \in \mathbb{R}^k$ such that $a_{ij} = \langle v_i,v_j \rangle$ for all $i,j$. 
\end{definition}

\begin{proposition}
A matrix is Gramian if and only if it is positive semidefinite.
\label{prop:gramian}
\end{proposition}

\begin{proposition}
Let $\phi : \rank \rightarrow \mathbb{R}^k$ be an embedding such that $\langle \phi(\sigma),\phi(\sigma) \rangle$ is independent of $\sigma$ (i.e., a constant). Then,
$$
\argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\| = \argmax_{\sigma \in \rank} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot \langle \phi(\sigma),\phi(\sigma') \rangle.
$$
\label{prop:equiv-dist-inner-product}
\end{proposition}
\begin{proof}
Let $\langle \phi(\sigma),\phi(\sigma) \rangle = c$ for all $\sigma \in \rank$. We have
\begin{align*}
&\argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\| \\
&= \argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\|^2 \\
&= \argmin_{\sigma \in \rank} \|\phi(\sigma)\|^2 + \|mean(\pi)\|^2 - 2 \cdot \langle \phi(\sigma),mean(\pi) \rangle  \\
&= \argmin_{\sigma \in \rank} c+\|mean(\pi)\|^2 - 2 \cdot \langle \phi(\sigma), mean(\pi) \rangle \\
&= \argmax_{\sigma \in \rank} \left\langle \phi(\sigma), \frac{1}{n} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot \phi(\sigma') \right\rangle \\
&= \argmax_{\sigma \in \rank} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot \langle \phi(\sigma),\phi(\sigma') \rangle. 
\end{align*}
\end{proof}

\begin{theorem}
A voting rule is a neutral symmetric mean proximity rule if and only if it is a neutral generalized scoring rule for which there exists a positive semidefinite score matrix with equal diagonal entries. 
\label{thm:psd}
\end{theorem}
\begin{proof}
Let $r$ be any neutral symmetric mean proximity rule. Then, by Lemma~\ref{lem:normalized-embedding}, it has a normalized embedding $\phi$. Recall that normalized embeddings satisfy $\langle \phi(\sigma),\phi(\sigma) \rangle = 1$ for all $\sigma \in \rank$. Thus, by Proposition~\ref{prop:equiv-dist-inner-product}, for any profile $\pi$,
$$
r(\pi) = \argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\| = \argmax_{\sigma \in \rank} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot \langle \phi(\sigma),\phi(\sigma') \rangle.
$$
It is now evident that the scoring function $s(\sigma,\sigma') = \langle \phi(\sigma),\phi(\sigma') \rangle$ represents rule $r$. Further, it corresponds to a Gramian score matrix, which is positive semidefinite (Proposition~\ref{prop:gramian}). Finally, the diagonal entries $\langle \phi(\sigma),\phi(\sigma) \rangle$ are equal to $1$ since $\phi$ is a normalized embedding.

On the contrary, take any neutral generalized scoring rule $r$ with a positive semidefinite (and hence Gramian) score matrix $S$ and equal diagonal entries. Thus, there exist vectors $v_1,\ldots,v_{m!} \in \mathbb{R}^k$ such that $S_{ij} = \langle v_i,v_j \rangle$. Take $\phi(\sigma_i) = v_i$ for all $i$. Since the diagonal entries are equal, we have that $\langle \phi(\sigma),\phi(\sigma) \rangle$ is independent of $\sigma$. Thus, Proposition~\ref{prop:equiv-dist-inner-product} implies that on any profile $\pi$, the rule returns 
\begin{align*}
r(\pi) = \argmax_{\sigma_i \in \rank} \sum_{\sigma_j \in \rank} n(\pi,\sigma_j) \cdot S_{ij} &= \argmax_{\sigma \in \rank} \sum_{\sigma' \in \rank} n(\pi,\sigma') \cdot \langle \phi(\sigma),\phi(\sigma') \rangle \\
&= \argmin_{\sigma \in \rank} \|\phi(\sigma)-mean(\pi)\|.
\end{align*}
This shows that $r$ is a symmetric mean proximity rule with embedding $\phi$. Note that $r$ is already neutral by assumption. 
\end{proof}

\cns{Ideal Goal: Find a score matrix satisfying both 1) and 2). Note that the score matrix of a linear embedding satisfies both. Improve 3) to be the corresponding neutral + normalized embedding.}

We now prove several properties of representations of a neutral symmetric mean proximity rule. 
\begin{lemma}
Any neutral symmetric mean proximity rule $r$ has the following special representations. 
\begin{enumerate}
\item A score matrix $S$ satisfying the following.
\begin{enumerate}
\item $S$ is positive semidefinite (hence, symmetric).
\item $S_{ii} = S_{jj}$ for all $i,j$, i.e., all diagonal entries of $S$ are equal.
\item $S_{ii} > S_{ij} = S_{ji}$ for all $i,j$, i.e., the largest element in any row or column is its diagonal element.
\item $\sum_j S_{ij} = \sum_{j} S_{ji} = 0$ for all $i$, i.e., the sum across each row and column is $0$. 
\end{enumerate}
\item A score matrix $S$ generated by a neutral scoring function $s$, i.e., $S_{ij} = s(\sigma_i,\sigma_j)$ for all $i,j$ and $s(\tau \sigma,\tau \sigma') = s(\sigma,\sigma')$ for all $\tau,\sigma,\sigma' \in \rank$.
\item A normalized embedding $\phi$ that satisfies $\|\phi(sigma)\| = 1$ for all $\sigma \in \rank$, and $\sum_{\sigma \in \rank} \phi(\sigma) = 0$.
\end{enumerate}
\label{lem:neutral-mean-proximity-properties}
\end{lemma}
\begin{proof}
For the first representation, take the score matrix $S$ obtained by inner products of a normalized embedding $\phi$ as in the proof of Theorem~\ref{thm:psd}. We already know that $S$ is positive semi-definite with equal diagonal entries. Condition (c) follows by unanimity of symmetric mean proximity rules (note that $S$ is symmetric, so $S_{ij} = S_{ji}$). Condition (d) follows since the sum across row (column) $i$ is $\langle \phi(\sigma_i),\sum_{\sigma \in \rank} \phi(\sigma) \rangle = 0$ as $\sum_{\sigma \in \rank} \phi(\sigma) = 0$ for any normalized embedding $\phi$. 

The second representation follows by Proposition~\ref{prop:gsr-neutral}, and the third representation follows by Lemma~\ref{lem:normalized-embedding}. 
\end{proof}


In the next section, we introduce a general class of embeddings such that all of them satisfy neutrality, and use Lemma~\ref{lem:neutral-mean-proximity-properties} to show that they generate all neutral symmetric mean proximity rules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Embeddings and Neutrality}

\cns{cite group theorey literature showing importance of linear representations and corresponding embeddings}

\begin{definition}[Linear Embeddings]
An embedding $\phi:\rank \rightarrow \mathbb{R}^k$ is called a \emph{linear embedding} if there exists a function $R : \rank \rightarrow \mathbb{R}^{k \times k}$ mapping each permutation to a $k \times k$ matrix such that i) $\phi(\tau \sigma) = R(\tau) \phi(\sigma)$ for every $\tau,\sigma \in \rank$, and ii) $R(\tau^{-1}) = R(\tau)^{-1} = R(\tau)^T$ for every $\tau \in \rank$. 
\end{definition}

Here, $\tau \sigma$ denotes the ranking obtained by permuting candidates in $\sigma$ according to $\tau$. In future, we will use $R_{\tau}$ in place of $R(\tau)$ for notational convenience. Note that linear embeddings are quite structured: embedding of $\sigma$ is obtained from embedding of $\sigma'$ by the linear transformation corresponding to the permutation that converts $\sigma'$ to $\sigma$. Correspondingly, we define linear symmetric mean proximity rules.

\begin{definition}[Linear Symmetric Mean Proximity Rule]
A voting rule is called a \emph{linear symmetric mean proximity rule} if there exists it has a symmetric mean proximity representation with a linear embedding. 
\end{definition}

First, we show that linearity is not a restrictive condition for symmetric mean proximity rules.

\begin{lemma}
All positional scoring rules and the Kemeny rule are linear symmetric mean proximity rules.
\label{lem:linear-captures}
\end{lemma}
\begin{proof}
\cns{To be written}
\end{proof}

Next, we prove one of the main results of this paper that shows the importance of linear mean proximity rules. 
\begin{theorem}
A mean proximity rule is neutral if and only if it is a linear mean proximity rule. 
\label{thm:linear-neutral}
\end{theorem}
\begin{proof}
\cns{One direction to be written, another to be proved}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%      UNPOLISHED SECTION       %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Connections to other approaches}
Voting rules have been analyzed from three viewpoints in the literature \cns{cite}: axiomatic view, distance rationalizability (DR) view, and MLE view. We connect linear mean proximity rules with all three viewpoints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Axiomatic View}
Proposition~\ref{prop:properties} shows that all mean proximity rules are consistent and connected. Theorem~\ref{thm:linear-neutral} shows that linear mean proximity rules are also neutral. This raises a few questions.

\begin{enumerate}
\item What restrictions on the embedding $\phi$ correspond to monotonicity of the mean proximity rule?
\item Caragiannis et. al.~\cite{CPS13} introduced pairwise majority consistency (PM-c) as the natural generalization of Condorcet consistency for social welfare functions. What restrictions on $\phi$ ensures that the mean proximity rule is PM-c? Is that related to the distance function $d(\sigma,\sigma') = \|\phi(\sigma)-\phi(\sigma')\|$ being majority concentric~\cite{CPS13}?
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Distance Rationalizability View}
Meskanen and Nurmi~\cite{MN08} introduced the distance rationalizability framework where given a consensus class of profiles where the output is already defined and a distance function between profiles, the corresponding distance rationalizable rule, given any profile, finds the closest profile in the consensus class and returns the output defined on that. Edith et. al.~\cite{EFS10} study additively votewise distances, where the distance between two profiles is (vaguely) the sum of distances between their rankings. We note that the additively votewise distance rationalizable rules with (strong) unanimity consensus class and the distance between two rankings being square of the Euclidean distance ($\ell_2$ norm) between their embeddings are exactly symmetric mean proximity. 

Note that while the Euclidean distance defines a distance metric (satisfying triangle inequality), its square may not be a distance metric. However, it turns out that for the pairwise comparison embedding that generates the Kemeny rule, the square of Euclidean distance is the Kendall Tau distance (up to a multiplicative constant), which is a distance metric. This has two important implications. First, it shows that under the more natural Euclidean distance function, Kemeny is indeed a \emph{mean} rule rather than a \emph{median} rule. This explains why the Kemeny rule has desirable properties, as the importance of mean consensus has been emphasized significantly in the literature. Second, it indicates that the distance metrics whose square are also distance metrics might be of independent interest, and might be connected to rules satisfying desirable properties. Another example of a distance metric whose square is also a distance metric is square root of the Hamming distance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{MLE View}
Given an embedding $\phi$, consider the noise model where 
$$
\Pr[\sigma | \sigma^*] = \frac{e^{-\|\phi(\sigma)-\phi(\sigma^*)\|^2}}{\sum_{\sigma' \in \rank} e^{-\|\phi(\sigma')-\phi(\sigma^*)\|^2}} \propto e^{-\|\phi(\sigma)-\phi(\sigma^*)\|^2}.
$$

This is a family of Gaussian noise models. Note that Mallows' model is a special case with $\phi$ being the pairwise comparison embedding. This indicates that Mallows' model is actually member of a Gaussian family. Further, define a distance function $d(\sigma,\sigma') = \|\phi(\sigma)-\phi(\sigma')\|$. When $d$ is neutral, i.e., $d(\tau \sigma,\tau \sigma') = d(\sigma,\sigma')$, the normalization is independent of $\sigma^*$. Note that this is indeed the case with linear embeddings. 

There are two interesting questions in this domain.
\begin{enumerate}
\item Mallows' model with the Kendall Tau distance admits an efficient sampling procedure. Is that the case with the family of Gaussian models introduced above when the embedding is linear? 
\item When we take the embedding for a positional scoring rule, we get a distribution where the probability of a ranking decreases exponentially as its discrepancy from the true ranking increases, and the discrepancy is measured by positions of alternatives rather than pairwise comparisons. Is there anything interesting about this distribution like Mallows' model?
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dimension of the Embeddings}

In this section, we analyze the following general question. \emph{Given a neutral symmetric mean proximity rule $r$, what is the minimum dimension (of the Euclidean space) required by any embedding representing $r$?}. For our main result of this section (Theorem~\ref{thm:dimension}), we need the following conjecture.
\begin{conjecture}
Let $r$ be any neutral symmetric mean proximity rule. Then, score matrices $S$ and $S'$ both represent $r$ if and only if there exist $a \in \mathbb{R}_{+}$ and $b \in \mathbb{R}^{m!}$ such that $S' = aS+B$, where $B = [b;\ldots;b]$ is the matrix where every row is $b$. 
\label{conj:equivalent-score-matrix}
\end{conjecture}
Note that all $aS+B$ transformations do preserve the underlying voting rule, because $(aS+B)y = a(Sy)+By$. Hence, $\argmax_{\sigma_i \in \rank} ((aS+B)y)_i = \argmax_{\sigma_i \in \rank} a(Sy)_i + by = \argmax_{\sigma_i \in \rank} (Sy)_i$. The other direction is still open.

\begin{theorem}
Let $S$ be any score matrix of a neutral symmetric mean proximity rule $r$. Let $c$ be the sum of elements of any row of $S$. Let $S'$ be the matrix such that $S'_{ij} = S_{ij}-c/{m!}$. Then, the minimum dimension required by any embedding representing $r$ is $rank(S')$. 
\label{thm:dimension}
\end{theorem}
\begin{proof}
\cns{Proved, to be written}
\end{proof}
%\begin{proof}
%Let $d$ be the minimum dimension achieved by any embedding representing $r$, and let $\phi$ be any such embedding. Let $B$ be the matrix whose rows are the $m!$ embeddings given by $\phi$. Then, by Equation~\eqref{eqn:equiv-dist-inner-product} in the proof of Theorem~\ref{thm:psd}, we know that $S' = B B^T$ is a score matrix of $r$. Also, $d \ge rank(B) = rank(B B^T = S')$. We first show that this inequality must in fact be an equality. This can be seen from the eigendecomposition of $S'$. Let $S' = U D U^T$. Then, $D$ is the diagonal matrix containing the eigenvalues. Only first $rank(S')$ diagonal entries are non-zero. Taking $B = U \sqrt{D}$ yeids an embedding of $r$, since $B B^T = U \sqrt{D} \sqrt{D} U^T = S'$. Further, last $m!-rank(S')$ columns of $B$ are zero. Hence, we can drop them to obtain an embedding with dimension $rank(S') \le d$. Since $d$ is the minimum dimension of any embedding representing $r$, we have that $d = rank(S')$.
%
%Now it is sufficient to show that for any score matrix $S$ of $r$, $rank(S') = rank([S;1])-1$. By Conjecture~\ref{conj:equivalent-score-matrix}, there exist $a \in \mathbb{R}_{+}$ and $b \in \mathbb{R}^{m!}$ such that $S' = aS+B$, where $B = [b;\ldots;b]$ is the matrix where every row is $b$. 
%
%
%Then, by fundamental properties of the rank of a matrix, we have $rank(S') \le rank([S;1])$. To see this, let $c_1,\ldots,c_{m!}$ represent the columns of $S$. Let $b = [b_1,\ldots,b_{m!}]$. Then, the columns of $S'$ are $a\cdot c_1+ b_1\cdot 1, \ldots, a\cdot c_{m!}+b_{m!}\cdot 1$. It is now easy to see the $column-space(S') \subseteq column-space([S;1])$, thus $rank(S') \le rank([S;1])$. 
%
%However, by Lemma~\ref{lem:neutral-mean-proximity-properties}, we have that the sum of all elements of every row of $S$ is the same, i.e., the sum of columns of $S$ is a multiple of the column vector $1$. Hence, $column-space(S) = column-space([S;1])$, i.e., $rank(S) = rank([S;1])$. Thus, we have $rank(S') \le rank(S)$. By symmetry, we can also achieve $rank(S) \le rank(S')$. Hence, we have that $rank(S) = rank(S')$. 
%
%Now, let $d$ be the score of (all) score matrices of $r$. Let $\phi$ be any embedding representing $r$ and let $d'$ be its dimension.
%\end{proof}

We investigate another connection. Let $\phi$ be any embedding of a neutral symmetric mean proximity rule $r$. Consider the embedding $\phi'(\sigma) = \phi(\sigma)-\phi_{avg}$, where $\phi_{avg} = (1/m!) \cdot \sum_{\sigma' \in \rank} \phi(\sigma')$. Then, the Gramian matrix of $\phi'$ (which is also a score matrix of $r$ since $\phi'$ embeds all rankings to points of unit norm) can be obtained from the score matrix of $\phi$ (the negation of the Euclidean distance matrix according to $\phi$) by the transformation described in Conjecture~\ref{conj:equivalent-score-matrix} in the unique way described in Theorem~\ref{thm:dimension}. 

For this, note that if $S$ and $S'$ are the negative Euclidean distance matrix and the Gramian matrix of $\phi$ and $\phi'$ respectively, then 
\begin{align*}
S_{ij} - 2 \cdot S'_{ij} &= -\|\phi(\sigma_i)-\phi(\sigma_j)\|^2 - 2 \cdot \langle \phi(\sigma_i)-\phi_{avg}, \phi(\sigma_j)-\phi_{avg}\rangle \\
&= -\|\phi(\sigma_i)\|^2 -\|\phi(\sigma_j)\|^2 + 2 \cdot \langle \phi(\sigma_i),\phi(\sigma_j) \rangle - 2 \cdot \langle \phi(\sigma_i),\phi(\sigma_j) \rangle \\
&\quad\quad + 2 \cdot \langle \phi(\sigma_i),\phi_{avg} \rangle + 2 \cdot \langle \phi(\sigma_j),\phi_{avg} \rangle - 2 \cdot \|\phi_{avg}\|^2 \\
&= -\left(\|\phi(\sigma_i)\|^2 - 2 \langle \phi(\sigma_i),\phi_{avg} + \|\phi_{avg}\|^2  \right)  \\
&\quad\quad - \left(\|\phi(\sigma_j)\|^2 - 2 \langle \phi(\sigma_j),\phi_{avg} + \|\phi_{avg}\|^2  \right) \\
&= - \|\phi(\sigma_i)-\phi_{avg}\|^2 - \|\phi(\sigma_j)-\phi_{avg}\|^2 = -2\cdot \lambda^2,
\end{align*}
where $\lambda = \|\phi(\sigma)-\phi_{avg}\|$ which is independent of $\sigma$ due to Lemma~\ref{lem:average-profile}. Thus, $S'_{ij} = (1/2) \cdot S_{ij} + \lambda^2$ for all $i,j$. Note that in the resulting matrix $S'$, the sum of any row $i$ is $\langle \phi'(\sigma_i),\phi'_{avg} \rangle$. However, $\phi'_{avg} = \phi_{avg}-\phi_{avg} = 0$. Hence, the sum of each row (and each column) is zero. Note that there is a unique way of obtaining a matrix with zero row and column sums via subtracting the same value from each element of the matrix, which is the transformation of Theorem~\ref{thm:dimension}. 

We just showed that the transformation of Theorem~\ref{thm:dimension}, when applied on a negative Euclidean distance matrix, is equivalent to the following three step transformation: i) take the embedding generating the matrix, ii) subtracting its average embedding from every embedding, iii) note that the final embedding has equal norm vectors, and take its Gramian matrix. Thus, we have the following alternative to Theorem~\ref{thm:dimension}: The minimum dimension required by any embedding representing a neutral symmetric mean proximity rule $r$ is equal to the rank of the Gramian matrix of any normalized embedding of $r$. 

Formally, let $r$ be any neutral symmetric mean proximity rule and $\phi$ be any embedding representing $r$. Let $\phi'$ be obtained by normalizing $\phi$. Then, the minimum dimension required by any embedding of $r$ is $rank(M)$, where $M_{ij} = \langle \phi'(\sigma_i), \phi'(\sigma_j) \rangle$ for any fixed enumeration $\sigma_1,\ldots,\sigma_{m!}$ of $\rank$. 


\cns{Interesting to consider: What are all possible normalized embeddings of a rule? It's not just rotations. Take two points on the circle within +/- 30 degree angle. Mapping them to any two points on that arc of the circle preserves the only neutral symmetric mean proximity rule: majority}

\begin{corollary}
The minimum dimension required by any embedding of a positional scoring rule over $m$ alternatives is $m-1$.
\end{corollary}
\begin{proof}
\cns{Proved, To be written.}
\end{proof}
%\begin{proof}
%Consider any positional scoring rule $r$ with arbitrary score vector $\alpha = [\alpha_1,\ldots,\alpha_m]$. Let $a_1,\ldots,a_m$ be any fixed enumeration of $A$. For any ranking $\sigma$ and $a \in A$, let $\sigma(a)$ denote the rank of $a$ in $\sigma$. Consider the positional embedding $\phi$ such that $\phi(\sigma) = [\alpha_{\sigma(a_1)},\ldots,\alpha_{\sigma(a_m)}]$ for all $\sigma \in \rank$. We already saw in Lemma~\ref{lem:symmetric-captures} that this corresponds to the positional scoring rule $r$. Let $B$ denote the matrix whose rows are embeddings of rankings according to $\phi$. Due to neutrality of positional scoring rules, $B B^T$ is a score matrix of $r$. Hence, the minimum dimension for this scoring rule is $rank(B B^T) = rank(B)$. 
%
%Now, note that $B$ has $m$ columns, and further by Lemma~\ref{lem:neutral-mean-proximity-properties}, the sum of all the columms is 
%\end{proof}

\begin{corollary}
The minimum dimension required by any embedding of the Kemeny rule over $m$ alternatives is $m\cdot(m-1)/2$.
\end{corollary}
\begin{proof}
\cns{Proved, To be written.}
\end{proof}

\subsection{Representations of the Borda Rule}
Zwicker~\cite{Zwicker08a} shows that the $m-1$ dimensional embedding for the Borda rule corresponds to a regular hexagon for $3$ alternatives. In fact, it is not too hard to observe (\cns{Already shown somewhere?}) that for $m$ alternatives, it is the regular polytope representing the permutahedron of the symmetric group $S_m$. It is also known (\cns{where?}) that the convex hull of the permutahedron is formed by connecting every ranking with all $m-1$ rankings at Kendall Tau distance $1$ from it (i.e., those obtained by a single adjacent swap). \emph{What interesting shapes do we get by replacing the Kendall Tau distance by other distances over rankings? What are the rules for such embeddings?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Embeddings of the Borda Rule}
%In this section, we analyze the $m-1$ dimension embeddings of the Borda rule. 
%
%{\bf $\mathbf{3}$ alternatives:} Zwicker~\cite{Zwicker08a} informally demonstrated that for $3$ alternatives, any embedding that represents the Borda rule embeds the $6$ possible rankings at the corners of a regular hexagon as shown in Figure~\ref{fig:borda-3alt}. In fact, it is exactly the \emph{permutahedron} of the symmetric group $S_3$ where each ranking is connected to two rankings that are obtained by two possible adjacent swaps. Thus, neighbouring rankings have \emph{Kendall Tau} distance $1$ from each other. Note that there are hexagons that connect rankings at KT distance greater than $1$, but those do not correspond to the Borda rule.
%
%\begin{figure}
%\centering
%\begin{subfigure}[$3$ alternatives]
%  %\includegraphics[width=.4\linewidth]{Borda3}
%  {\rule{3cm}{3cm}}
%  \label{fig:borda-3alt}
%\end{subfigure}%
%\begin{subfigure}[$4$ alternatives]
%  %\includegraphics[width=.4\linewidth]{Borda4}
%  {\rule{3cm}{3cm}}
%  \label{fig:borda-4alt}
%\end{subfigure}%
%\caption{Embeddings for the Borda Rule}
%\label{fig:borda}
%\end{figure}
%
%{\bf $\mathbf{4}$ alternatives:} Interestingly, the observation that the Borda rule embeds rankings to the vertices of the regular polytope of the permutahedron carries over to the case of $4$ alternatives. Thus, each ranking is connected to $3$ rankings that are obtained by performing one of the three possible adjacent swaps. The polytope of the permutahedron of $S_4$, shown in Figure~\ref{fig:borda-4alt}, consists of $8$ hexagons and $6$ squares. Each hexagon contains the $6$ rankings that are obtained by either keeping the first alternative constant or the last alternative constant. Note that we can forget the alternative that is constant, and the hexagon exactly matches the embeddings of the rankings over the remaining $3$ alternatives. The $6$ squares each contain $4$ rankings that are obtained by swapping the first two or the last two alternatives. Thus, each square has the following form: 
%$$
%a \succ b \succ c \succ d \;\longrightarrow\; a \succ b \succ d \succ c \;\longrightarrow\; b \succ a \succ d \succ c \;\longrightarrow\; b \succ a \succ c \succ d.
%$$
%{\bf Replace this by a figure of a square, similarly draw a hexagon}
%
%For better understanding of permutahedrons, refer to Crisman~\cite{Crisman}. 
%
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Other Research Questions}
\begin{enumerate}
\item What shape, and what corresponding voting rule do we get if we replace the KT distance by some other distance in the permutahedron?
\item Proving a lower bound on the dimensions for the Kemeny rule.
\item Alternative proof of the characterization of positional scoring rules by observing that they are obtained by the representation $R$ where $R_{\tau}$ is the permutation matrix generated by $\tau$ (which permutes rankings according to the mapping $\sigma \rightarrow \tau \sigma$). (This representation and all possible initial embedding $\phi$). 
\item Conjecture by Conitzer et. al.~\cite{CRX09}: Consistent $+$ continuous $+$ neutral $\Leftrightarrow$ Neutral SRSF.
\item What are the equivalence classes of $\phi$ that lead to the same voting rule?
\item What about notions of consensus in Euclidean spaces other than mean $\rightarrow$ e.g., minimize maximum distance (everyone ``lets go'' equally)?
\end{enumerate}

\section{Scribbled Notes}
These are just high level intuitions. Need further investigations.
\begin{enumerate}
\item If the construction of Conitzer works then,
\begin{itemize}
\item We don't need equal diagonal in the theorem. We can take any PSD, do the construction, and obtain equivalent matrix with equal diagonals.
\item We can take the matrix in Lemma 6 (1) and apply that construction. If we prove that construction preserves PSD, then not only we get a matrix satisfying both (1) and (2), but also its Gramian decomposition will give us a neutral normalized embedding. 
\end{itemize}
\item We can approach the other way by constructing a neutral normalized embedding (enough to obtain any neutral embedding, normalization preserves it) and take its score matrix. 

\item We can prove that linear normalized embeddings are also enough. Normalization preserves linearity by the same representation since $R_{\tau} \phi_{avg} = \phi_{avg}$ must hold.

\item Neutral $\Rightarrow$ all rows permutations of each other $\Rightarrow$ sum = constant $\Leftrightarrow$ $[1,\ldots,1]$ is an eigenvector $\Leftrightarrow$ first coordinate of the embedding constant $\Leftrightarrow$ there is an embedding of rank-1

\item What is the permutation of all rankings generated by applying $\tau$ to each of them? What are all these $m!$ permutations?

\item Anything interesting about inner product maximizing rules that do not have embeddings of equal norm? Even symmetric ones may not be unanimous. 

\item An illustration: (I think) the \emph{Least Frequent Ranking Rule} is not a symmetric mean proximity rule. One score matrix is negative identity matrix. Can possibly show that no score matrix is PSD. Note that no $aS+B$ transformation can help.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
Mean proximity rule / generalized scoring rule / SRSF - Neutral $\Rightarrow$ SRSF iff MLE
{\bf Question:} (Linear) Mean Proximity Rules - Captures all ``pairwise comparison scoring rules''?
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{abbshort,ultimate}
\end{document}